\documentclass[a4paper, 11pt]{article}

%Comandos para configurar el idioma
\usepackage[spanish,activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} %Necesario para el uso de las comillas latinas.
\usepackage{geometry}
\usepackage{graphicx}

%Importante que esta sea la última órden del preámbulo
\usepackage{hyperref}

\newcommand\fnurl[2]{%
\href{#2}{#1}\footnote{\url{#2}}%
}

\newcommand{\includecode}[2][c]{\lstinputlisting[caption=#2, escapechar=, style=custom#1]{#2}<!---->}

%Paquetes matemáticos
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[all]{xy} %Para diagramas
\usepackage{enumerate} %Personalización de enumeraciones
\usepackage{tikz} %Dibujos

%Tipografía escalable
\usepackage{lmodern}
%Legibilidad
\usepackage{microtype}

%Código
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\lstset{
    backgroundcolor=\color{lbcolor},
    tabsize=4,
    rulecolor=,
    language=[GNU]C++,
    basicstyle=\scriptsize,
    upquote=true,
    aboveskip={1.5\baselineskip},
    columns=fixed,
    showstringspaces=false,
    extendedchars=false,
    breaklines=true,
    prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
    frame=single,
    numbers=left,
    showtabs=false,
    showspaces=false,
    showstringspaces=false,
    identifierstyle=\ttfamily,
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.026,0.112,0.095},
    stringstyle=\color[rgb]{0.627,0.126,0.941},
    numberstyle=\color[rgb]{0.205, 0.142, 0.73
%   \lstdefinestyle{C++}{language=C++,style=numbers}’.
}
\{
    backgroundcolor=\color{lbcolor},
    tabsize=4,
    language=C++,
    captionpos=b,
    tabsize=3,
    frame=lines,
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    breaklines=true,
    showstringspaces=false,
    basicstyle=\footnotesize,
    %  identifierstyle=\color{magenta},
    keywordstyle=\color[rgb]{0,0,1},
    commentstyle=\color[rgb]{0.026,0.112,0.095}, % Darkgreen
    stringstyle=\color{red},
    escapeinside={(*@}{@*)}
}
% Slightly bigger margins than the latex defaults

\geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\setlength{\parskip}{.5em} % por defecto el espacio entre párrafos es 0pt

\theoremstyle{definition}
\newtheorem{ejercicio}{Ejercicio}
\newtheorem{cuestion}{Cuestión}
\newtheorem*{solucion}{Solución}
\newtheorem*{bonus}{BONUS}

%%%%%%%% New sqrt
\usepackage{letltxmacro}
\makeatletter
\let\oldr@@t\r@@t
\def\r@@t#1#2{%
\setbox0=\hbox{$\oldr@@t#1{#2\,}$}\dimen0=\ht0
\advance\dimen0-0.2\ht0
\setbox2=\hbox{\vrule height\ht0 depth -\dimen0}%
{\box0\lower0.4pt\box2}}
\LetLtxMacro{\oldsqrt}{\sqrt}
\renewcommand*{\sqrt}[2][\ ]{\oldsqrt[#1]{#2} }
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\hypersetup{
    pdftitle={Informe de proyecto: Implementación del algoritmo de rectificación de Loop \& Zhang},
    pdfauthor={Antonio Álvarez Caballero, Alejandro García Montoro},
    unicode,
    breaklinks=true,  % so long urls are correctly broken across lines
    colorlinks=true,
    urlcolor=blue,
    linkcolor=blue,
    citecolor=darkgreen,
}

\title{Informe de proyecto: \\ Implementación del algoritmo de rectificación de Loop \& Zhang}
\author{Antonio Álvarez Caballero \\ Alejandro García Montoro \\
\href{mailto:analca3@correo.ugr.es}{analca3@correo.ugr.es} \\
\href{mailto:agarciamontoro@correo.ugr.es}{agarciamontoro@correo.ugr.es}}
\date{}
%%%%%%%%%%%%%%%%% FIN PREAMBULO %%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

    \maketitle

    \section{Descripción del problema}

    La rectificación de imágenes es el proceso de aplicar sendas homografías a un par de imágenes cuya geometría epipolar conocemos, de modo que las líneas epipolares queden horizontales, paralelas entre sí y con la misma coordenada vertical.

    La motivación de la rectificación es simple: al tener las líneas epipolares horizontales y con la misma coordenada vertical, buscar correspondencias entre las dos imágenes es mucho más fácil y eficiente; como sabemos que las correspondencias se encuentran en las mismas líneas epipolares, si suponemos que las imágenes están rectificadas basta buscar en la correspondiente fila de píxeles de la otra imagen, donde se encuentra la línea epipolar del punto que estemos considerando. Si las imágenes no estuvieran rectificadas, el tiempo de ejecución aumentaría considerablemente; la búsqueda en líneas inclinadas ---no paralelas con ninguno de los dos ejes--- es más compleja computacionalmente.

    Así, es claro que tener un par de imágenes rectificadas mejora la eficiencia de muchos de los algoritmos que precisan conocer la geometría epipolar y tienen que buscar correspondencias entre ambas imágenes. Por ejemplo, de esta situación se consigue la mejora computacional de los algoritmos que calculan mapas de disparidad, donde hay que medir el desplazamiento relativo entre los píxeles de una y otra imagen. Esta distancia relativa es lo que conocemos por \emph{disparidad}, y con esta información se puede reconstruir la profundidad incluso sin noción alguna de cámaras; basta un par de imágenes de una misma escena estática.

    Este proyecto implementa el método de rectificación propuesto por Charles Loop y Zhengyou Zhang en \cite{LoopZhang}.

    Su algoritmo es totalmente geométrico y no precisa conocimiento de las cámaras.
    Se consigue descomponiendo la homografía que se desea calcular para cada imagen en una proyección, una transformación euclídea y una cizalla, con especial cuidado en minimizar la distorsión del resultado con respecto a las imágenes originales.

    Es de hecho este último criterio el que hace del algoritmo una de las mejores soluciones actuales, ya que se consigue una rectificación que mantiene casi todas las propiedades de las imágenes originales.


    \section{Enfoque de la implementación y detalles de eficiencia}

    La implementación del algoritmo se basa en la descomposición de la homografía $H$ que se quiere obtener en las siguientes transformaciones:
    \begin{itemize}
        \item $H_p$: transformación proyectiva.
        \item $H_r$: transformación de semejanza.
        \item $H_s$: composición de una transformación de cizalla con un escalado y una traslación.
    \end{itemize}

    \subsection{Descomposición}
    Siguiendo la notación de \cite{LoopZhang}, sea $H$ la homografía siguiente:
    \[
    H =
    \begin{pmatrix}
        u_a & u_b & u_c \\
        v_a & v_b & v_c \\
        w_a & w_b & w_c
    \end{pmatrix}
    \]

    Si descomponemos $H$ en su parte proyectiva y afín ---y esta a su vez en una parte de semejanza y otra de cizalla---, obtenemos que
    \[
    H = H_s H_r H_p
    \]
    donde
    \[
    H_s =
    \begin{pmatrix}
        s_a & s_b & s_c \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix};\;\;
    H_r =
    \begin{pmatrix}
        v_b - v_c w_b & v_c w_a - v_a & 0 \\
        v_a - v_c w_a & v_b - v_c w_b & v_c \\
        0 & 0 & 1
    \end{pmatrix};\;\;
    H_p =
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        w_a & w_b & 1
    \end{pmatrix}
    \]

    El cálculo de cada transformación se hace por separado, teniendo en cuenta las matrices calculadas anteriormente y criterios de minimización que se explicarán en cada subsección.

    Antes de todo este cálculo, sin embargo, hace falta calcular la geometría epipolar del par de imágenes, que el algoritmo supone conocida. Este paso previo se ha implementado buscando las correspondencias con un detector \lstinline{BRISK}, calculando la matriz fundamental con la llamada a la función \lstinline{findFundamentalMat} de \emph{OpenCV}---usando el algoritmo de los 8 puntos y \lstinline{RANSAC}--- y usando las llamadas a \lstinline{computeCorrespondEpilines} de la misma librería.

    \subsection{Transformación proyectiva}
    La transformación proyectiva lleva los epipolos $e$ y $e'$ al infinito, de manera que tras aplicar $H_p$ y $H'_p$, las líneas epipolares son ya paralelas.

    El cálculo de $w_a$, $w_b$ y  $w'_a$, $w'_b$ se basa en un criterio de minimización de la distorsión que puede resumirse en lo siguiente: buscamos unas transformaciones proyectivas $H_p$ y $H'_p$ que sean lo más cercanas que podamos a transformaciones afines.

    La idea es que las líneas $w = [w_a, w_b, 1]$ y $w' = [w'_a, w'_b, 1]$ transformen los puntos de la manera más compensada posible; es decir, dado un punto $p_i = [p_{i,u}, p_{i,v}, 1]$, la transformación $H_p$ lo transformará en el punto $[\frac{p_{i,u}}{\omega_i}, \frac{p_{i,v}}{\omega_i}, 1]$, donde los pesos $\omega_i$ pueden medir de alguna manera cuán proyectiva es nuestra transformación: si todos los pesos son idénticos, la transformación es afín.

    Buscamos por tanto unas líneas $w$ y $w'$ que consigan que los pesos $\omega_i$ se parezcan todo lo posible.

    Toda la discusión matemática se encuentra descrita en \cite{LoopZhang}, transformando el problema propuesto en la minimización de la siguiente expresión:
    \begin{equation}
        \frac{z^TAz}{z^TBz} + \frac{z^TA'z}{z^TB'z}
        \label{eq_minz}
    \end{equation}

    donde las matrices $A$, $B$ y sus homólogas con primas son conocidas y el vector $z = [\lambda, 1, 0]$ depende de un único parámetro $\lambda$ y es tal que $w = [e]_times z$ y $w' = Fz$.

    El cálculo de las matrices $A$, $B$, $A'$ y $B'$ depende únicamente de por qué matriz se multiplica: $[e]_\times$ si queremos calcular $A$ y $B$ o la matriz fundamental $F$ si queremos calcular $A$ y $B$:
    \begin{align*}
        A = [e]_\times^T P P^T [e]_\times && A' = F^T P' P'^T F \\
        B = [e]_\times^T p_c p_c^T [e]_\times && B' = F^T p_c' p_c'^T F
    \end{align*}

    Este cálculo se encuentra implementado en la siguiente función, donde la matriz \lstinline{mult_mat} representa $[e]_\times$ o $F$ según si queremos calcular las versiones normales o las primas:
    \begin{lstlisting}
    void obtainAB(const Mat &img, const Mat &mult_mat, Mat &A, Mat &B){
        int width = img.cols;
        int height = img.rows;

        int size = 3;

        Mat PPt = Mat::zeros(size, size, CV_64F);

        PPt.at<double>(0,0) = width*width - 1;
        PPt.at<double>(1,1) = height*height - 1;

        PPt *= (width*height) / 12.0;

        double w_1 = width - 1;
        double h_1 = height - 1;

        double values[3][3] = {
            {w_1*w_1, w_1*h_1, 2*w_1},
            {w_1*h_1, h_1*h_1, 2*h_1},
            {2*w_1, 2*h_1, 4}
        };

        Mat pcpct(size, size, CV_64F, values);

        pcpct /= 4;
        A = mult_mat.t() * PPt * mult_mat;
        B = mult_mat.t() * pcpct * mult_mat;
    }
    \end{lstlisting}

    Desarrollando la expresión descrita en \ref{eq_minz} obtenemos un polinomio en $\lambda$ cuyo mínimo es el valor que buscamos. Este mínimo se alcanza cuando su derivada con respecto a $\lambda$ es cero, así que todo nuestro problema se reduce a encontrar esta raíz.

    \subsubsection{Primera aproximación a la raíz}

    En \cite{LoopZhang} se da una primera aproximación a esta raíz, cuyo cálculo consiste en minimizar por separado ambos sumandos y tomar la media normalizada de las dos soluciones.

    De nuevo, allí se encuentra toda la discusión matemática, que se reduce a hacer lo siguiente para ambos sumandos:
    \begin{itemize}
        \item Hacer la descomposición de Cholesky de la matriz $A$ (resp. $A'$) ---que es simétrica y definida positiva--- para obtener una matriz triangular superior $D$ tal que $A = D^TD$  (resp. $A' = D'^TD'$).
        \item Encontrar el vector propio $y$ (resp. $y'$) asociado al mayor valor propio de $(D^{-1})^T B D^{-1}$ (resp. $(D'^{-1})^T B' D'^{-1}$).
        \item Tomar $z_1 = D^{-1}y$ (resp. $z_2 = D'^{-1}y'$)
    \end{itemize}

    Se toma entonces como primera aproximación a la raíz el siguiente vector:
    \[
    \frac{1}{2}\left(\frac{z_1}{\lVert z_1 \rVert} + \frac{z_2}{\lVert z_2 \rVert}\right)
    \]

    Toda este cálculo se encuentra implementado en la siguiente función:
    \begin{lstlisting}
    Vec3d getInitialGuess(Mat &A, Mat &B, Mat &Ap, Mat &Bp){

        Vec3d z_1 = maximize(A, B);
        Vec3d z_2 = maximize(Ap, Bp);

        return (normalize(z_1) + normalize(z_2))/2;
    }
    \end{lstlisting}

    donde el código realmente interesante está en \lstinline{maximize}:
    \begin{lstlisting}
    Vec3d maximize(Mat &A, Mat &B){
        Mat D; // Output of cholesky decomposition: upper triangular matrix.
        if( choleskyCustomDecomp(A, D) ){

            Mat D_inv = D.inv();

            Mat DBD = D_inv.t() * B * D_inv;

            // Solve the equations system using eigenvalue decomposition

            Mat eigenvalues, eigenvectors;
            eigen(DBD, eigenvalues, eigenvectors);

            // Take largest eigenvector
            Mat y = eigenvectors.row(0);

            Mat sol = D_inv*y.t();

            Vec3d res(sol.at<double>(0,0), sol.at<double>(1,0), sol.at<double>(2,0));

            return res;
        }

        // At this point, there is an error!
        Mat eigenvalues;
        eigen(A, eigenvalues);

        cout << "\n\n\n----------------------------- WARNING -----------------------" << endl;

        return Vec3d(0, 0, 0);
    }
    \end{lstlisting}

    Aunque \emph{OpenCV} pone a disposición del usuario una función que realiza el cálculo de la descomposición de Cholesky, su uso en este trabajo introducía errores de redondeo que impedían realizar la descomposición. En ocasiones calculaba valores propios muy cercanos a cero ---del orden de $10^{-5}$--- que tomaba como negativos, comprobando entonces erróneamente que la matriz $A$ no era definida positiva.

    Para evitar estos fallos y tener total control sobre los cálculos, decidimos implementar una función propia que calculara esta descomposición. La función es la que sigue, e implementa las fórmulas descritas en \cite{wikiChol}.
    \begin{lstlisting}
    bool choleskyCustomDecomp(const Mat &A, Mat &L){

      L = Mat::zeros(3,3,CV_64F);

      for (int i = 0; i < 3; i++){
        for (int j = 0; j <= i; j++){
          double sum = 0;
          for (int k = 0; k < j; k++){
            sum += L.at<double>(i,k) * L.at<double>(j,k);
          }

          L.at<double>(i,j) = A.at<double>(i,j) - sum;
          if (i == j){
            if (L.at<double>(i,j) < 0.0){
              if (L.at<double>(i,j) > -1e-5){
                L.at<double>(i,j) *= -1; (*@\label{comprobacion}@*)
              }
              else{
                return false;
              }
            }
            L.at<double>(i,j) = sqrt(L.at<double>(i,j));
          }
          else{
            L.at<double>(i,j) /= L.at<double>(j,j);
          }
        }
      }

      L = L.t();

      return true;
    }
    \end{lstlisting}

    La línea \ref{comprobacion} es la que evita los errores de redondeo descritos anteriormente. Cuando se intenta realizar la raíz cuadrada de un elemento, primero se comprueba si es negativo. En caso de serlo, se debe lanzar un fallo, a no ser que el valor sea muy cercano a cero; en esa situación, simplemente tomamos el valor opuesto y seguimos el algoritmo con naturalidad.

    En este punto del proceso tenemos una primera aproximación a la raíz que necesitamos, muy cercana a la real según \cite{LoopZhang} pero que puede ser optimizada.

    \subsubsection{Optimización de la raíz}
    El problema de encontrar una raíz de un polinomio está intensamente documentado en la literatura, y uno de los mejores algoritmos para abordar este problema es el de Newton-Raphson.

    Para implementarlo, hace falta simplemente la función cuya raíz queremos encontrar, su derivada y una primera aproximación.

    La función cuya raíz queremos encontrar es la derivada con respecto a $\lambda$ de la expresión \ref{eq_minz}. Esta función, tomando como parámetros $A$, $B$, $A'$ y $B'$ es la siguiente:
    \begin{align*}
    f(\lambda) &= \frac{2A'_{0,0}\lambda+A'_{1,0}+A'_{0,1}}{\lambda(B'_{0,0}\lambda+B'_{0,1})+B'_{1,0}\lambda+B'_{1,1}} \\
    &- \frac{(2B'_{0,0}\lambda+B'_{1,0}+B'_{0,1})(\lambda(A'_{0,0}\lambda+A'_{0,1})+A'_{1,0}\lambda+A'_{1,1})}{(\lambda(B'_{0,0}\lambda+B'_{0,1})+B'_{1,0}\lambda+B'_{1,1})^2} \\
    &+ \frac{2A_{0,0}\lambda+A_{1,0}+A_{0,1}}{\lambda(B_{0,0}\lambda+B_{0,1})+B_{1,0}\lambda+B_{1,1}} \\
    & - \frac{(2B_{0,0}\lambda+B_{1,0}+B_{0,1})(\lambda(A_{0,0}\lambda+A_{0,1})+A_{1,0}\lambda+A_{1,1})}{(\lambda(B_{0,0}\lambda+B_{0,1})+B_{1,0}\lambda+B_{1,1})^2}
    \end{align*}

    Basta entonces implementar esta función, su derivada ---cuya expresión no representamos aquí por ser demasiado larga--- y el método de Newton-Raphson para optimizar la primera aproximación de la raíz que obtuvimos en el paso anterior. Todo este cálculo numérico de notación obtusa pero razonamiento evidente, se encuentra encapsulado en la siguiente función:
    \begin{lstlisting}
    void optimizeRoot(const Mat &A, const Mat &B,
                       const Mat &Ap, const Mat &Bp,
                       Vec3d &z){

        double lambda = z[0];

        z[0] = NewtonRaphson(A,B,Ap,Bp, lambda);
        z[1] = 1.0;
        z[2] = 0.0;
    }
    \end{lstlisting}

    La función interesante es \lstinline{NewtonRaphson}, cuyo código es el siguiente:
    \begin{lstlisting}
    double NewtonRaphson(const Mat &A, const Mat &B,
                         const Mat &Ap, const Mat &Bp,
                         double init_guess){
        double current = init_guess;
        double previous;

        double fx = function(A,B,Ap,Bp, current);
        double dfx = derivative(A,B,Ap,Bp, current);

        int iterations = 0;

        do {
            previous = current;
            current = current - fx / dfx;

            fx = function(A,B,Ap,Bp, current);
            dfx = derivative(A,B,Ap,Bp, current);

            iterations++;
        } while (abs(fx) > 1e-15 && iterations < 150);
        // Double-precision values have 15 stable decimal positions

        return current;
    }
    \end{lstlisting}
    donde \lstinline{function} y \lstinline{derivative} son las funciones $f(\lambda)$ y $\frac{d}{d\lambda}f(\lambda)$, cuyo código se puede encontrar en el fichero \lstinline{utils.cpp}.

    Del algoritmo de Newton-Raphson cabe mencionar su criterio de parada: se busca un elemento $\alpha$ tal que $f(\alpha) \in ]-\varepsilon, \varepsilon[$, donde $\varepsilon = 10^{-15}$. La elección de $\varepsilon$ se debe a la precisión de los \lstinline{double}: son 15 las posiciones decimales que el estándar considera estables ---de hecho, la precisión es de 15.95 posiciones decimales, así que hay casi un decimal más de precisión---; como esto es lo mínimo que podemos medir con rigor, es un buen valor como criterio de parada. En todo caso, si este valor no se alcanzara ---en ocasiones la raíz no cambia de una iteración a otra y aún no se ha entrado en el intervalo $]-\varepsilon, \varepsilon[$---, el algoritmo se detiene tras 150 iteraciones.

    \subsubsection{Construcción de la homografía}
    Una vez se ha optimizado la raíz con el método de Newton-Raphson, basta tomar las siguientes líneas:
    \begin{align*}
        w &= [e]_\times z \\
        w' &= F z
    \end{align*}
    y usar la primera y segunda coordenadas de las mismas para construir la matriz, como se hace en el siguiente extracto del código de \lstinline{main.cpp}:
    \begin{lstlisting}
    // Get w
    Mat w = e_x * Mat(z);
    Mat wp = fund_mat * Mat(z);

    w /= w.at<double>(2,0);
    wp /= wp.at<double>(2,0);

    cout << "w = " << w << endl;
    cout << "wp = " << wp << endl;

    // Get final H_p and Hp_p matrix for projection
    Mat H_p = Mat::eye(3, 3, CV_64F);
    H_p.at<double>(2,0) = w.at<double>(0,0);
    H_p.at<double>(2,1) = w.at<double>(1,0);

    Mat Hp_p = Mat::eye(3, 3, CV_64F);
    Hp_p.at<double>(2,0) = wp.at<double>(0,0);
    Hp_p.at<double>(2,1) = wp.at<double>(1,0);
    \end{lstlisting}

    En este momento tenemos sendas homorgafías $H_p$ y $H'_p$, tan cercanas a una homorgafía afín como es posible, que transforman los epipolos $e$ y $e'$ a puntos del infinito.

    \subsection{Transformación de semejanza}
    Euclídea

    \subsection{Transformación de cizalla}

    \section{Experimentos realizados}

    Lola es muy guapa y adorable. V impone.

    \section{Valoración de resultados}

    Todo se ve de lujo

    \section{Conclusiones}

    Este algoritmo es pro. Mejor que el de OpenCV.

\end{document}
